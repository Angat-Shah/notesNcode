# -*- coding: utf-8 -*-
"""CIFAR-10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ma_ZIJByq8ZjEAps7LR7zxnP5ETlVeE4

# CNN on CIFAR-10
CIFAR-10 dataset contains 32x32 color images from 10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck:
<img src="https://stephan-osterburg.gitbook.io/~gitbook/image?url=https%3A%2F%2F4138123170-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-legacy-files%2Fo%2Fassets%252F-LRrOFNeUGLZef_2NLZ0%252F-LeEJi2MCK6d2wToNmIy%252F-LeENgJqZAzBJi8PRELl%252Fcifar10.png%3Falt%3Dmedia%26token%3Dbee1e40d-b4f6-4f46-a835-43d3f1223166&width=400&dpr=3&quality=100&sign=875c685f&sv=1" style="width:100%" />

# Import required libraries
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.regularizers import l2
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

"""# Load and preprocess CIFAR-10 data"""

# Load the data
(x_train, y_train), (x_test, y_test) = cifar10.load_data()

# Normalize pixel values between 0 and 1
x_train, x_test = x_train / 255.0, x_test / 255.0

# One-hot encode the labels
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

print("Data shapes:")
print("x_train shape:", x_train.shape)
print("y_train shape:", y_train.shape)
print("x_test shape:", x_test.shape)
print("y_test shape:", y_test.shape)

"""# Display sample image of each class from the training dataset"""

NUM_CLASSES = 10
cifar10_classes = ["airplane", "automobile", "bird", "cat", "deer", "dog", "frog", "horse", "ship", "truck"]

# Set up a figure with 2 rows and 5 columns
cols = 5
rows = 2
fig, axes = plt.subplots(rows, cols, figsize=(10, 6))

# Find and store the first instance of each class in the training set
class_sample_indices = [np.where(np.argmax(y_train, axis=1) == i)[0][0] for i in range(NUM_CLASSES)]

# Plot each class sample
for idx, (ax, class_idx) in enumerate(zip(axes.flat, class_sample_indices)):
    ax.imshow(x_train[class_idx])
    ax.set_title(cifar10_classes[idx])
    ax.axis('off')

plt.tight_layout()
plt.show()

"""# Implement Data Augmentation"""

datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True
)

# Fit the datagen on training data
datagen.fit(x_train)

"""# Define CNN model"""

def create_cnn_model():
    model = Sequential()

    # Block 1
    model.add(Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001), input_shape=(32, 32, 3)))
    model.add(BatchNormalization())
    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
    model.add(BatchNormalization())
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.3))

    # Block 2
    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
    model.add(BatchNormalization())
    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
    model.add(BatchNormalization())
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.4))

    # Block 3
    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
    model.add(BatchNormalization())
    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
    model.add(BatchNormalization())
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.5))

    # Fully connected layer
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(BatchNormalization())
    model.add(Dropout(0.5))
    model.add(Dense(10, activation='softmax'))

    return model

# Initialize the model
model = create_cnn_model()
model.summary()

"""# Compile the model"""

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

"""# Train the model

Define callback and fit data in model
"""

# Using callbacks to improve convergence and prevent overfitting
early_stopping = EarlyStopping(monitor='val_accuracy', patience=50, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)

# Train the model
history = model.fit(
    datagen.flow(x_train, y_train, batch_size=64),
    epochs=100,
    validation_data=(x_test, y_test),
    validation_split=0.2,
    callbacks=[early_stopping, reduce_lr]
)

"""# **Model Evaluation**

# Plot the training history

 Plotting accuracy with loss
"""

# Plot accuracy
plt.plot(history.history['accuracy'], label='train accuracy')
plt.plot(history.history['val_accuracy'], label='val accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Plot loss
plt.plot(history.history['loss'], label='train loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""# Evaluate the model on the test set"""

test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)
print(f"Test Accuracy: {test_accuracy:.4f}")
print(f"Test Loss: {test_loss:.4f}")

"""# Evaluating via classification report"""

# Predict classes and evaluate with classification report and confusion matrix
y_pred = np.argmax(model.predict(x_test), axis=-1)
y_true = np.argmax(y_test, axis=-1)

# Classification report
print("\nClassification Report:")
print(classification_report(y_true, y_pred))

"""# Evaluating via Confusion Matrix"""

# Confusion Matrix
conf_matrix = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

"""# Generate predictions and display random test samples with predictions and probabilities"""

y_pred = model.predict(x_test)

# Convert predictions to class labels and max probabilities for display
y_pred_test_classes = np.argmax(y_pred, axis=-1)
y_pred_test_max_probabs = np.max(y_pred, axis=-1)

# Plotting test images with model predictions
cols = 8
rows = 2
fig = plt.figure(figsize=(2 * cols - 1, 3 * rows - 1))
for i in range(cols):
    for j in range(rows):
        random_index = np.random.randint(0, len(y_test))
        ax = fig.add_subplot(rows, cols, i * rows + j + 1)
        ax.grid(False)
        ax.axis('off')
        ax.imshow(x_test[random_index])

        # Display predicted label, probability score, and true label
        pred_label = cifar10_classes[y_pred_test_classes[random_index]]
        pred_proba = y_pred_test_max_probabs[random_index]
        true_label = cifar10_classes[np.argmax(y_test[random_index])]
        ax.set_title("Pred: {}\nScore: {:.3f}\nTrue: {}".format(
            pred_label, pred_proba, true_label
        ))

plt.show()

"""---

### Practical Summary: Convolutional Neural Network (CNN) for CIFAR-10 Classification


#### Steps and Key Components

1. **Data Preparation**:
   - Loaded the CIFAR-10 dataset, containing 60,000 32x32 color images across 10 classes.
   - Preprocessed the data by normalizing pixel values to the range [0, 1] for faster training convergence.
   - Created data augmentations (e.g., rotation, shifts, and flips) to improve model generalization.

2. **Model Architecture**:
   - Designed a CNN with multiple convolutional and max-pooling layers:
     - Used **Conv2D** layers with ReLU activation and increasing filters (e.g., 32, 64, 128).
     - Applied **MaxPooling2D** layers to down-sample and reduce feature dimensions.
     - Added **Dropout** layers (ranging from 0.3 to 0.5) to reduce overfitting by randomly disabling neurons during training.
   - Output layer uses a **Softmax** activation with 10 units to classify images into CIFAR-10 classes.

3. **Model Training**:
   - Used **EarlyStopping** (patience = 50 epochs) to stop training when validation accuracy stopped improving.
   - Implemented a **ReduceLROnPlateau** callback to dynamically adjust the learning rate and stabilize convergence.
   - Achieved a final training accuracy of around 84.39% and validation accuracy of ~86.13%.

4. **Evaluation and Analysis**:
   - Assessed model performance on test data and displayed random predictions with probability scores.
   - Observed some overfitting (higher training than validation accuracy) and mitigated it using:
     - Increased dropout,
     - L2 regularization,
     - Data augmentation to generate more varied training samples.


**Conclusion**: The CNN model achieved effective classification on CIFAR-10 with an accuracy of 86% on the validation set, meeting the target performance.

---
"""

